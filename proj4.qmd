---
title: "Bias in Machine Risk Assessment Tools"
description: |
  Analysis of Bias and Ethical Dilemmas in Machine Risk Assessment Tools
author: Minsong Kim
date: December 9, 2025
format: html
---

# Ethical Dilemma: Machine Risk Assessment Tools in Criminal Justice

The ethical data dilemma examined here centers on COMPAS (Correctional Offender Management Profiling for Alternative Sanctions), a widely used machine risk-assessment tool in the U.S. criminal justice system. COMPAS, trained on large amounts of data, uses unique algorithms that chooses how to interpret that data and is used in all steps of the criminal justice systems in counties across the country, including the pretrial process, in determining who should be given bonds, who should be incarcerated, who should be given parole, etc (Angwin et al. 2016). The data component of this issue is about how this machine tool is created and maintained. Large amounts of data is needed to train COMPAS, and how the creator chooses which data to feed and train COMPAS off of, how the data is interpreted, which data is given weight and which isn't are all very relevant components of data science. The ethical dilemma is very integrally tied to the data science component of this issue. In their current state, this risk assessment tool often have little to no transparency as to how they assess data or compute risk scores for defendants. Additionally, its validity as an "unbiased" tool is contested, and they have the potential to very easily exacerbate and increase the systemic discrimination present in the today's criminal justice system of the United states (Chohlas-Wood 2020).

## Who Benefits? Who is harmed?

The for-profit company, Northpointe, benefits. They are allowed to profit off a tool that could be harmful on a systemic level by citing trade-secret claims (Angwin et al. 2016). The current oppressors and people who benefit from systemic discrimination benefits from this tool that exacerbates it. It matters because by adopting a tool that is reinforcing systemic discrimination, we are taking two steps back in trying to take one step forward. The ethical violations were because in the interest of profit. A lot of the criticisms would not apply to an open source project for a risk-assessment tool.

## Data Collection and Quality

We don't know how the variables that go into calculating risk scores are selected, cleaned, or chosen, and that is one of the primary concerns with the current state of this machine risk-assessment tool. There is little to no transparency with COMPAS under the guise of trade-secrets (Chohlas-Wood 2020), and no accountability for the creators as such. Because these tools are being run by a for-profit entity, Northpointe, they are motivated to mask their process and ensure the public does not know how their algorithms work (Angwin et al. 2016). However, that means Northpointe also face no accountability or responsibility for how they choose the data they feed and train their tool off of, nor for the systemic discrimination they may be exacerbating. Historical data of criminal justice often have extremely strong racial biases built in due to the flawed and discriminatory nature of the societal system of the time (Chohlas-Wood 2020).

## Use Beyond Its Intended Purpose

The COMPAS tool is currently being used in ways contrary to the original intention. Tim Brennan, a former statistics professor and co-creator of COMPAS, has said that he did not intend for the tool to be used in courts, especially to decide punishment (Angwin et al. 2016). Yet this is often how the tool is used, as it is used in this manner in multiple counties such as La Crosse County, Wisconsin (Angwin et al. 2016).

## Use of Race and Sex as Variables

COMPAS does not directly include race as an input, but it may indirectly capture racial information through correlated variables such as prior arrests or neighborhood factors. Including race is ethically problematic because race is a social construct with no biological basis (Morey 2023); using it risks reinforcing existing inequalities. In contrast, sex has and should be included because sex is a biological category with measurable differences between males and females. However, regardless of variable type, the absence of transparency about how these factors are weighted remains a major ethical concern.

## Alignment with Data Values and Principles

One principle of responsible data science is the creation of reproducible and extensible work. The proprietary criminal justice tool named COMPAS violate this principle: its algorithms and documentation are inaccessible, preventing replication, peer review, or public oversight due to Northpointe's profit motive. I believe this to be a large issue. A tool such as this used in our criminal justice system, which has large societal impacts for generations, should not be maintained by a for-profit company, nor should their development be curtailed because of a for-profit motive, when it should be peer-reviewed and open source.

# Bibliography

Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” ProPublica. ProPublica. May 23, 2016. <https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing>.

Chohlas-Wood, Alex . 2020. “Understanding Risk Assessment Instruments in Criminal Justice.” Brookings. 2020. <https://www.brookings.edu/articles/understanding-risk-assessment-instruments-in-criminal-justice/>.

Morey, Rajendra. 2023. “What Is the Biological Basis for Race - Implications for Psychiatric Genetic.” *European Neuropsychopharmacology* 75 (October): S47–47. <https://doi.org/10.1016/j.euroneuro.2023.08.095>.
