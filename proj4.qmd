---
title: "Bias in Machine Risk Assessment Tools"
description: |
  Analysis of Bias and Ethical Dilemmas in Machine Risk Assessment Tools
author: Minsong Kim
date: November 12, 2025
format: html
---

First, find an ethical dilemma with a data science component. Please take on 
only one ethical dilemma, it is too difficult to compare multiple dilemmas in 
one short blog entry. There are many examples below, and within each example 
you should start with the reference provided and find at least one more article 
(possibly from another angle? or go find the privacy policy / user agreement if 
there is one!) to expand your understanding of the topic. It should be clear 
from your report what information came from which article. Feel free to choose 
an example different from those below.


Describe the example / scenario as if to someone who is not at all familiar 
with the setting. In particular, it should be clear both what is the data 
science component and what is the ethical dilemma.

The ethical data dilemma that I am exploring today has to do with the usage of
machine risk-assessment tools in criminal justice. These tools, trained on large
amounts of data, with unique algorithms that chooses how to interpret that data
are used in all steps of the criminal justice systems in counties across the 
country, including the pretrial process, who should be given bonds, who should
be incarcerated, who should be given parole, etc (ProPublica) The data component 
of this issue is about how these machine tools are created and maintained. Large 
amounts of data is needed to train these tools, and how creators choose which 
data to feed and train off of, how the data is interpreted, which data is given 
weight and which isn't are all very relevant components of data science. The 
ethical dilemma is very integrally tied to the data science component of this 
issue. In their current state, these risk assessment tools often have little to
no transparency as to how they assess data or compute risk scores for
defendants. Additionally, their validity as an "unbiased" tool is contested, and 
they have the potential to very easily exacerbate and increase the systemic
discrimination present in the today's criminal justice system of the United
states (Brookings).


Respond to at least 4 of the items below (from the list of questions or the 
Data Values and Principles Manifesto). Four separate paragraphs that explain 
both the issue (e.g., consent) and how the issue played out in the data science 
example. Note: totally fine if there are items that were done well in your 
example.


Given what you described in #3 (above), summarize by explaining why it matters. 
Who benefits? Who is neglected or harmed? Were the ethical violations in the 
interest of profit? Surveillance? Power?

The for-profit company benefits. They are allowed to profit off a tool that 
could be harmful on a systemic level by citing trade-secret claims. The current
oppressors and people who benefit from systemic discrimination benefits from 
this tool that exacerbates it. It matters because by adopting a tool that is
reinforcing systemic discrimination, we are taking two steps back in trying to
take one step forward. The ethical violations were because in the interest of 
profit. A lot of the criticisms would not apply to an open source project for
a risk-assessment tool.


How were the variables collected? Were they accurately recorded? Is there any 
missing data?

We don't know, and that is one of the primary concerns with the current state of
these machine risk-assessment tools. There is little to no transparency with 
these tools and creators under the guise of trade-secrets (Brookings), and no
accountability for the creators as such. Because these tools were created with
the significant part of the purpose being profit, these creators are motivated
to mask their process and ensure the public does not know how their algorithms 
work. However, that means these creators also face no accountability or 
responsibility for how they choose the data they feed and train their tool off
of, nor for the systemic discrimination they may be exacerbating. Historical
data of criminal justice often have extremely strong racial biases built in due
to the flawed and discriminatory nature of the societal system of the time 
(Brookings).


Is the data being used in unintended ways to the original study?

The data is 100% being used in unintended ways to the original intention. One of
the specific risk-assessment tools that is often brought up in these discussions
is the Correctional Offender Management Profiling for Alternative Sanctions, or
COMPAS. Tim Brennan, a former statistics professor and co-creator of COMPAS, has
said that he did not intend for the tool to be used in courts, especially to 
decide punishment (ProPublica). Yet this is often how the tool is used, as it 
is used in this manner in multiple counties such as La Crosse County, Wisconsin
(ProPublica). 


Should race be used as a variable? Is it a proxy for something else (e.g., 
amount of melanin in the skin, stress of navigating microaggressions, zip-code, 
etc.)? What about gender?

I argue that race should not be a variable, but gender should be. The reason
I argue that race should not be a variable in these risk-assessment tools is 
because there has yet to be a causational, peer-reviewed study that shows a 
significant difference between races in traits that could matter. When the 
burden of proof has not been sustained for the argument that race is a relevant
factor for risk-assessment, I do not believe it should be a factor that is 
considered. In addition, there is also the issue that race is largely a social 
construct, especially how it is percieved in today's society. How can something
that is societally constructed, and can thus be changed with the social whims 
of society cause a biological difference between human individuals? The
argument feels a little bit ridiculous to me. However, from what I read from
the two articles, it feels like these risk-assessment tools are not utilizing
race as a direct factor, but is indirectly measuring race by utilizing factors
that are strongly correlated to race (ProPublica, Brookings). This feels like a 
much more difficult issue to me because these factors, such as poverty and 
joblessness to name two, are definitely factors that would strongly influence
risk factors. Gender feels different because there are biological explanations
for the statically higher rate of recriminalization for men compared to women
(Brookings), such as higher testosterone. 


Data Values and Principles manifesto

As data teams, we aim to…

Use data to improve life for our users, customers, organizations, and 
communities.

Create reproducible and extensible work.

These tools, which are made and maintained by for profit companies, are not
reproducible, nor is their documentation publicly extensive. I believe that
to be a large issue. A tool such as this used in our criminal justice system,
which has large societal impacts for generations, should not be maintained by
a for-profit company, nor should their development be curtailed because of a 
for-profit motive, when it should be peer-reviewed and open source.


Build teams with diverse ideas, backgrounds, and strengths.

Prioritize the continuous collection and availability of discussions and 
metadata.

Clearly identify the questions and objectives that drive each project and use 
to guide both planning and refinement.

Be open to changing our methods and conclusions in response to new knowledge.
Recognize and mitigate bias in ourselves and in the data we use.

Present our work in ways that empower others to make better-informed decisions.

Consider carefully the ethical implications of choices we make when using data, 
and the impacts of our work on individuals and society.

Respect and invite fair criticism while promoting the identification and open 
discussion of errors, risks, and unintended consequences of our work.

Protect the privacy and security of individuals represented in our data.

Help others to understand the most useful and appropriate applications of data 
to solve real-world problems.


Credit:

Angwin, Julia, Jeff Larson, Lauren Kirchner, and Surya Mattu. “Machine Bias.” ProPublica, May 23, 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. 

Denisa Gándara, Hadis Anahideh, Megan Stevenson Jennifer L. Doleac, Virginia Foggo John Villasenor, John R. Allen Darrell M. West, Ruby Bolaria Shifrin Rosanne Haggerty, Renée Rippberger Norman Eisen, and Michelle Du Josie Stewart. “Understanding Risk Assessment Instruments in Criminal Justice.” Brookings, June 27, 2023. https://www.brookings.edu/articles/understanding-risk-assessment-instruments-in-criminal-justice/. 