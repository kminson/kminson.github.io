[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "My About Page",
    "section": "",
    "text": "Hey there, welcome to My About page!\nMy name is Minsong Kim, and I am a student at Pomona College at the time of writing this page. I have a love-hate relationship with math and school, and I enjoy exploring and messing around with data! I’m from South Korea, and I’ve got a dog and a cat.\nHow about a fun fact? This year (at the time of writing this page), 2025, is a perfect sum of the first ten cubes!\n\n\n0^3 + 1^3 + 2^3 + 3^3 + 4^3 + 5^3 + 6^3 + 7^3 + 8^3 + 9^3\n\n[1] 2025\n\n\nYes, I’m counting 0 as one of the first ten cubes. I think the phrase “first ten cubes” is a lot cooler than “first nine cubes”. Maybe in 3025 I’ll use this fun fact again with the “first ten cubes” again, but this time with 10 instead of 0."
  },
  {
    "objectID": "proj5.knit.html",
    "href": "proj5.knit.html",
    "title": "Exploring/Data Wrangling in SQL",
    "section": "",
    "text": "For this project I am going to be exploring the SQL database of the Stanford Open Policing Project, published in Pierson et al. (2020). This main purpose of this project is to practice using SQL code, and as such, I will be doing all data wrangling in SQL, using R only to visualize the data. I decided to explore the relationship of time and number of traffic stops made in three different states, Washington, Ohio, and New York. I split up the day into 6 time bins, as such.\nLate Night: 12:00AM - 4:00AM\nEarly Morning: 4:00AM - 8:00AM\nMorning: 8:00AM - 12:00PM\nMidday: 12:00PM - 4:00PM\nAfternoon: 4:00PM - 8:00PM\nEvening: 8:00PM - 12:00AM\nTo begin, I connect to the SQL database and begin wrangling the data from the database to output a table with only the information that I need to create my visuals.\n\n\nCode\n#Connect to the SQL Database\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)\n\n\n\n\nCode\n-- Data wrangling in SQR to get the table with just the data we need\n\nSELECT state, time_bin, COUNT(*) AS stops \nFROM (\n-- Creating a new variable called time_bin which categorizes each stop depending\n-- on its time of day. We do this for Washington, New York, and Michigan.\n  -- Washington Data\n  SELECT 'WA' AS state,\n    CASE \n      WHEN HOUR(time) BETWEEN 0 AND 3 THEN 'Late Night'\n      WHEN HOUR(time) BETWEEN 4 AND 7  THEN 'Early Morning'\n      WHEN HOUR(time) BETWEEN 8 AND 11 THEN 'Morning'\n      WHEN HOUR(time) BETWEEN 12 AND 15 THEN 'Midday'\n      WHEN HOUR(time) BETWEEN 16 AND 19 THEN 'Afternoon'\n      WHEN HOUR(time) BETWEEN 20 AND 23 THEN 'Evening'\n      ELSE 'Unknown'\n    END AS time_bin\n  FROM wa_statewide_2020_04_01\n  \n  UNION ALL\n  \n  -- New York Data\n  SELECT 'NY' AS state,\n    CASE \n      WHEN HOUR(time) BETWEEN 0 AND 3 THEN 'Late Night'\n      WHEN HOUR(time) BETWEEN 4 AND 7  THEN 'Early Morning'\n      WHEN HOUR(time) BETWEEN 8 AND 11 THEN 'Morning'\n      WHEN HOUR(time) BETWEEN 12 AND 15 THEN 'Midday'\n      WHEN HOUR(time) BETWEEN 16 AND 19 THEN 'Afternoon'\n      WHEN HOUR(time) BETWEEN 20 AND 23 THEN 'Evening'\n      ELSE 'Unknown'\n    END AS time_bin\n  FROM ny_statewide_2020_04_01\n  \n  UNION ALL\n  \n  -- Ohio Data\n  SELECT 'OH' AS state,\n    CASE \n      WHEN HOUR(time) BETWEEN 0 AND 3 THEN 'Late Night'\n      WHEN HOUR(time) BETWEEN 4 AND 7  THEN 'Early Morning'\n      WHEN HOUR(time) BETWEEN 8 AND 11 THEN 'Morning'\n      WHEN HOUR(time) BETWEEN 12 AND 15 THEN 'Midday'\n      WHEN HOUR(time) BETWEEN 16 AND 19 THEN 'Afternoon'\n      WHEN HOUR(time) BETWEEN 20 AND 23 THEN 'Evening'\n      ELSE 'Unknown'\n    END AS time_bin\n  FROM oh_statewide_2020_04_01\n) t\n\nGROUP BY state, time_bin\n\n\nAfterwards, I try two different methods of visualizing the data to see which would work better for comparing and contrasting the patterns between the three states. First, I try visualizing each state in a bar graph of their own, and put them side by side to compare and contrast the data from each state.\n\n\nCode\ntimeofday_tibble &lt;- timeofday_tibble |&gt;\n  mutate(time_bin = fct_relevel(time_bin, \n                                c(\"Late Night\", \"Early Morning\", \"Morning\", \"Midday\", \"Afternoon\", \"Evening\")\n                                ))\n\nggplot(timeofday_tibble, aes(x = time_bin, y = stops)) +\n  geom_col() +\n  facet_wrap(~ state) +\n  labs(title = \"Traffic stops by time-of-day (counts)\",\n       x = \"Time of Day\",\n       y = \"Number of stops\") +\n  theme(axis.text.x = element_text(angle = 25))\n\n\n\n\n\n\n\n\n\nMy second attempt is combining all the data into one bar graph, with three entries for each time bin, one for each state. The states are differentiated by color.\n\n\nCode\nggplot(timeofday_tibble, aes(x = time_bin, y = stops, fill = state)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Traffic stops by time-of-day (counts)\",\n    x = \"Time of Day\",\n    y = \"Percentage of Stops\",\n    fill = \"State\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nI find that its easier to compare and contrast the overall trends in each state when we visualize the data in three seperate graphs right next to each other. The combined graph becomes a little bit too noisy to be able to clearly pick out trends and patterns in each state, but it’d be useful if we wanted to directly compare the counts for each time bin between states rather then comparing the overall pattern.\nLooking at the overall data, we see a general pattern, and some interesting tidbits for each state. Generally, traffic stops are low from midnight til 8:00am, where it spikes and stays high until 4:00pm. where it starts decreasing. Ohio is the only state out of the three that maintains a high number of traffic stops during the late night time bin (12:00AM til 4:00AM), while Washington and New York have similarly low counts of traffic stops for both the late night and early morning time bin. New York, seems to have a significant dip in the traffic stops in the afternoon time bin for whatever reason, something we don’t see for Ohio and Washington.\n\n\nCode\nDBI::dbDisconnect(con_traffic, shutdown = TRUE)\n\n\nCredit:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minsong Kim’s Projects in R",
    "section": "",
    "text": "Hello! My name is Minsong Kim, first of my name, child of Chonghui Pak and Chaepil Kim. This is my website with my projects made with delight where I explore various data sets and R functionalities to practice and have fun with my coding skills!\n(Pictured above is my friend’s fish slippers)"
  },
  {
    "objectID": "tidytue1.html",
    "href": "tidytue1.html",
    "title": "Pokemon",
    "section": "",
    "text": "Every pokemon has a list of base stats that correspond to their abilities. A pokemon with a base speed stat of 10 will be faster than a pokemon with a base speed state of 5. Base stat totals are one way to roughly measure a pokemon’s total power level, by adding up the stat totals across all their stats. Generally, if a pokemon with a higher base stat total will be stronger than a pokemon with a lower one. A pokemon’s egg group also classifies what kind of pokemon it is. A dragon egg group pokemon will be pokemon that is dragon-like in some way, while a flying egg group pokemon might be a bird of some sort. I also created another measurement called size index, which takes a pokemon’s height and weight, and multiplies them together to create a unit of measurement that measures both the size and the density of a pokemon roughly. I then plotted every pokemon by their size index versus their base stat totals to see if there was a correlational relationship between the two. I also colored each pokemon by their primary egg group and created a regression line for each egg group so we can see a pattern for each individual egg group and compare them with one another.\n\n\nCode\npokemon_df |&gt;\n  mutate(bst = hp + attack + defense + special_attack + special_defense + speed) |&gt;\n  mutate(size_index = height * weight)|&gt;\n  \n  ggplot(aes(x = size_index, y = bst, color = egg_group_1)) +\n    geom_point(size = 2.0) +\n    scale_x_log10(labels = comma) +\n    geom_smooth(method = \"lm\", linewidth = 1.2, se = FALSE) +\n    labs(\n      x = \"Size Index (Height * Weight)\",\n      y = \"Base Stat Total\",\n      title = \"Size Index vs. Base Stat Total\",\n      color = \"Egg Group\"\n    ) +\n    theme(\n    text = element_text(size = 40),      \n    axis.title = element_text(size = 40),\n    axis.text = element_text(size = 30)\n  )\n\n\n\n\n\n\n\n\n\nWe can see that a positive relationship between the size index and base stat total exists for every egg group, but the steepness of the relationship changes depending on the egg group. I think on a design level, that makes sense. You want pokemon who are super big to be stronger than a pokemon the size of your hand generally; otherwise the design contradicts the reality of the strength of the pokemon, as we humans expect big things to be proportionally stronger than small things.\nCredit:\nDataset Original Source:\nhttps://bulbapedia.bulbagarden.net/wiki/Main_Page\nhttps://github.com/phalt/pokeapi\nhttps://pokemon-uranium.fandom.com/wiki/Main_Page\nhttps://github.com/HybridShivam/Pokemon\nTidyTuesday Page:\nhttps://github.com/rfordatascience/tidytuesday/tree/main/data/2025/2025-04-01\nDataset Curated by Frank Hull:\nhttps://github.com/frankiethull"
  },
  {
    "objectID": "proj2.html",
    "href": "proj2.html",
    "title": "Obama’s Tweets",
    "section": "",
    "text": "For this project, I’ve decided to take a look at former President Obama’s tweets while he was a president. Twitter is a social media platform. As you might have guessed from the name, its all about being social. Now, how social was Obama when he served as the US president? We’ll take a look at his mentioning habits.\n\n\nCode\ndetect_username &lt;- \"(?&lt;=@)\\\\w{1,15}\" #Setting the regex to detect\n#usernames that come after @\nobama_tweets &lt;- obama_tweets |&gt;\n  mutate(\n    #Create a new column listing all mentioned usernames in the\n    #tweet entry\n    mentioned_user = str_extract_all(text, detect_username),\n         mentioned_user = \n           map(mentioned_user, ~if (length(.x) == 0){\n             NA_character_}\n             else .x),\n    #Create a new column whether or not the tweet has a mention\n         has_mention = if_else(str_detect(text, detect_username),\n                               \"Yes\", \"No\"), \n         has_mention = factor(has_mention, levels = c\n                              (\"Yes\", \"No\")),\n    #Create a column that counts exclamation marks in tweet\n         excl_count = str_count(text, \"!+\"),\n    #Create a column that counts question marks in tweet\n         ques_count = str_count(text, \"\\\\?+\"),\n    #Create a column that classifies tweet as an\n    #Exclamation, Question, Both, or Neither\n         sentence_type = ifelse\n         (excl_count &gt; 0 & ques_count == 0, \"Exclamation\",\n           ifelse \n           (ques_count &gt; 0 & excl_count == 0, \"Question\",\n             ifelse \n             (excl_count &gt; 0 & ques_count &gt; 0, \"Both\", \"Neither\")\n             )\n           ),\n    #Refactor the sentence_type variable to reorder the possible\n    #values in a way that makes sense\n         sentence_type = factor(sentence_type, levels = c\n                                (\"Exclamation\",\n                                  \"Question\",\n                                  \"Both\",\n                                  \"Neither\"))\n         )\n\n\n\n\nCode\n#Create a new tibble called user_proportions that list every user\n#mentioned by Obama and the number of times mentioned\nuser_proportions &lt;- obama_tweets |&gt;\n  select(mentioned_user) |&gt;\n  unnest(mentioned_user) |&gt;\n  filter(!is.na(mentioned_user)) |&gt;\n  count(mentioned_user) |&gt;\n  mutate(proportion = n / sum(n)) |&gt;\n  filter(!n == 1) |&gt;\n  mutate(mentioned_user = fct_reorder(mentioned_user, proportion))\n\n#Create a tibble called mention_summary with number of tweets\n#with mentions and tweets without, and the proportion of each\nmention_summary &lt;- obama_tweets |&gt;\n  count(has_mention) |&gt;\n  mutate(proportion = n / sum(n))\n\n#Create a tibble called style_data with number of tweets for \n#style of tweet as defined previously, and the proportion of\n#each\nstyle_data &lt;- obama_tweets |&gt;\n  count(sentence_type) |&gt;\n  mutate(proportion = n / sum(n))\n\n\n\n\nCode\nmention_summary |&gt;\n  ggplot(aes(x = has_mention, y = proportion, fill = has_mention)) +\n  geom_col() +\n  labs(\n    title = \"Proportion of Obama's tweets that include a mention\",\n    x = \"Did the tweet include a mention?\",\n    y = \"Proportion\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFrom this table, we can see that President Obama didn’t really spend most of his time interacting with other users on Twitter. This includes all sorts of stuff like re tweeting other people’s tweets, or replying to tweets, since those things require the user to mention the original tweeter. It does make sense, since he was the President of the United States, so stuff like PR was probably very important to him, and he didn’t really have an incentive to engage other users. Still, try to be a bit less anti-social!\n\n\nCode\nuser_proportions |&gt;\n  ggplot(aes(\n    x = proportion, y = mentioned_user, fill = mentioned_user\n    )) +\n  geom_col() +\n  labs(\n    title = \"Obama's Most Mentioned Users\",\n    x = \"Proportion of Total Mentions\",\n    y = \"Username\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\",)\n\n\n\n\n\n\n\n\n\nLets take a look at who enjoyed President Obama’s mentions the most. I filtered out any username that Obama only mentioned once, since it cluttered up the visual and isn’t really important to see who Obama likes to interact with. But, man, isn’t this too little to be all the users Obama mentioned more than once? In fact, he tends to mention the user @POTUS much more than any other user, including his political ally @HillaryClinton, or @FLOTUS. It makes up nearly 10% of all his mentions! Makes Obama look a little bit like a clout chaser….\n\n\nCode\nstyle_data |&gt;\n  ggplot(aes(x = sentence_type, y = proportion, fill = sentence_type)) +\n  geom_col() +\n  labs(\n    title = \n      \"Proportion of Obama's Tweets as Exclamations/Questions\",\n    x = \"Style of tweet\",\n    y = \"Proportion\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 12)\n    )\n\n\n\n\n\n\n\n\n\nNow lets take a look at what kind of style of tweets Obama likes to use. I classified tweets based on whether the tweet included an exclamation mark, a question mark, both, or neither. We can see that most of Obama’s tweets use neither of those punctuation marks, but he uses a lot more exclamation marks than question marks. He comes off a little bit like a mansplainer when we take a look at the data like this, especially since he rarely asks questions. Obama, its okay to admit when you don’t know something.\nCredit:\nData Sourced from:\nhttps://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media\nby the National Archives"
  },
  {
    "objectID": "proj3.html",
    "href": "proj3.html",
    "title": "Monty Hall Simulation",
    "section": "",
    "text": "On this page, we will be exploring the Monty Hall problem through simulations. The Monty Hall problem is a probability problem that follows as such. Say you are on a game show, where you are given three doors to choose from. One door has a car, which represents winning the game, while the other two have goats, which represents failing the game. You select one door at random. Then, the host reveals one of the doors you didn’t select, revealing a goat, and gives you the choice of either switching to the other door or staying with your initial choice. Should you choose to switch or not? The problem got its name from the original host of the TV show it was based off of, Let’s Make A Deal hosted by Monty Hall. This problem has a very interesting history, where a genius female columnist, by the name of Marilyn Savant, gave an answer and posted it on a newspaper. What followed was thousands of messages and mail telling her she was wrong, many with PhD’s, when she was in fact correct (Wikipedia Contributors 2019). Now lets see what the answer is!\nFirst we begin by coding a function called monty_hall that represents the game once. The function has one input that determines if the player chooses to switch or not.\n\n\nCode\nmonty_hall &lt;- function(switch_choice) {\n  doors &lt;- letters[1:3] #A character vector of a, b, c to represent the three doors; a is the prize door\n  first_choice &lt;- sample(doors, 1) #Randomly choose our first door\n  \n  \n  reveal &lt;- if (first_choice == \"a\") { #Randomly reveal either b or c if a is selected\n    sample(doors[doors != \"a\"], 1)\n  } else if (first_choice == \"b\") { #Otherwise, reveal the unselected non-prize door\n    \"c\"\n  } else {\n    \"b\"\n  }\n  \n  doors &lt;- doors[doors != reveal] #Reveal the selected door\n  \n  second_choice = if (switch_choice == 1) { \n    second_choice = sample(doors[doors != first_choice], 1) #If switch = 1, we switch to the other door\n  } else {\n    second_choice = first_choice #Otherwise, we stay with our first door\n  }\n\n  result = ifelse(second_choice == \"a\", #If our second choice is a, we win!\n                         \"Success!\",\n                         \"Dud...\"\n                  )\n  data.frame(second_choice, result) #Create a dataframe with a column showing final selected door and results\n}\n\n\nThen we map the function to a numeric vector that spans from 1 to 100,000, effectively running the function 100,000 times. This is our simulation.\n\n\nCode\nn &lt;- 100000 #Set the simulation count to 100,000\nswitch_result &lt;- 1:n |&gt; #Use mapping to simulate the function n times with switching\n  map(~ monty_hall(1)) |&gt;\n  list_rbind()\n\nswitch_summary &lt;- switch_result |&gt; #Condense the table of results to proportion of successes and failures\n  group_by(result) |&gt;\n  summarise(number = n()) |&gt;\n  mutate(proportion = number / sum(number))\n\nnoswitch_result &lt;- 1:n |&gt; #Repeat for no switching\n  map(~ monty_hall(0)) |&gt;\n  list_rbind()\n\nnoswitch_summary &lt;- noswitch_result |&gt;\n  group_by(result) |&gt;\n  summarise(number2 = n()) |&gt;\n  mutate(proportion2 = number2 / sum(number2))\n\n\nWe’ll take the results of this simulation and work it into a table for plotting, and finally create a visual with our results!\n\n\nCode\nfull_summary &lt;- full_join(switch_summary, noswitch_summary, by = \"result\") #Join the proportion results into one table\n\nplot_data &lt;- full_summary |&gt; #Create a new table for easier plotting\n  select(result, proportion, proportion2) |&gt;\n  pivot_longer(cols = starts_with(\"proportion\"),\n               names_to = \"strategy\",\n               values_to = \"proportion\") |&gt;\n  mutate(strategy = if_else(strategy == \"proportion\", \"Switch\", \"No Switch\")) |&gt;\n  mutate(result = fct_relevel(result, c(\"Success!\", \"Dud...\"))) |&gt;\n  mutate(strategy = fct_relevel(strategy, c(\"Switch\", \"No Switch\")))\n\nggplot(plot_data, aes(x = result, y = proportion, fill = strategy)) + #Plot!\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Monty Hall Simulation Results of 100,000 Runs\",\n    x = \"Outcome\",\n    y = \"Proportion of Wins\",\n    fill = \"Strategy\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs you can see by the table, when the player chooses to switch, they have roughly 2/3rd odds to win, while only have 1/3rd odds to win if they choose not to switch. I was surprised because, even though I knew from previously reading about this problem that it was to the player’s advantage to switch, I had a misunderstanding as to why that was. I believed that by revealing a losing door, the remaining unselected door had 1/2 odds of being the winning door since there were only two doors left, but the initial door only had 1/3rd odds of winning since it was selected while there was still 3 doors. However, the correct logic is that by switching, the unselected door gives the player 2/3rd odds of winning, as it effectively gives the player the “odds” of both the unrevealed unselected door and the revealed unselected door. When Savant wrote her explanation to her answer, she used this analogy.\n“Suppose there are a million doors, and you pick door #1. Then, the host, who knows what’s behind the door and will always avoid the one with a prize, opens them all except door #777,777. You’d switch to that door pretty fast, wouldn’t you?”\nI like this analogy a lot, because by scaling the problem to the extreme, Savant makes the solution a lot easier to grasp intuitively then it is with the problem’s original setup. There’s actually a whole Wikipedia page on this problem and all the research it spawned. The page has stuff like psychological tendacies why people intuitively grasp the wrong solution, how to mathematically prove the solution of the problem, different variants, etc. It’s all very fascinating stuff.\nCredit:\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem"
  },
  {
    "objectID": "proj5.html",
    "href": "proj5.html",
    "title": "Exploring/Data Wrangling in SQL",
    "section": "",
    "text": "For this project I am going to be exploring the SQL database of the Stanford Open Policing Project, published in Pierson et al. (2020). This main purpose of this project is to practice using SQL code, and as such, I will be doing all data wrangling in SQL, using R only to visualize the data. I decided to explore the relationship of time and number of traffic stops made in three different states, Washington, Ohio, and New York. I split up the day into 6 time bins, as such.\nLate Night: 12:00AM - 4:00AM\nEarly Morning: 4:00AM - 8:00AM\nMorning: 8:00AM - 12:00PM\nMidday: 12:00PM - 4:00PM\nAfternoon: 4:00PM - 8:00PM\nEvening: 8:00PM - 12:00AM\nTo begin, I connect to the SQL database and begin wrangling the data from the database to output a table with only the information that I need to create my visuals.\n\n\nCode\n#Connect to the SQL Database\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)\n\n\n\n\nCode\n-- Data wrangling in SQR to get the table with just the data we need\n\nSELECT state, time_bin, COUNT(*) AS stops \nFROM (\n-- Creating a new variable called time_bin which categorizes each stop depending\n-- on its time of day. We do this for Washington, New York, and Michigan.\n  -- Washington Data\n  SELECT 'WA' AS state,\n    CASE \n      WHEN HOUR(time) BETWEEN 0 AND 3 THEN 'Late Night'\n      WHEN HOUR(time) BETWEEN 4 AND 7  THEN 'Early Morning'\n      WHEN HOUR(time) BETWEEN 8 AND 11 THEN 'Morning'\n      WHEN HOUR(time) BETWEEN 12 AND 15 THEN 'Midday'\n      WHEN HOUR(time) BETWEEN 16 AND 19 THEN 'Afternoon'\n      WHEN HOUR(time) BETWEEN 20 AND 23 THEN 'Evening'\n      ELSE 'Unknown'\n    END AS time_bin\n  FROM wa_statewide_2020_04_01\n  \n  UNION ALL\n  \n  -- New York Data\n  SELECT 'NY' AS state,\n    CASE \n      WHEN HOUR(time) BETWEEN 0 AND 3 THEN 'Late Night'\n      WHEN HOUR(time) BETWEEN 4 AND 7  THEN 'Early Morning'\n      WHEN HOUR(time) BETWEEN 8 AND 11 THEN 'Morning'\n      WHEN HOUR(time) BETWEEN 12 AND 15 THEN 'Midday'\n      WHEN HOUR(time) BETWEEN 16 AND 19 THEN 'Afternoon'\n      WHEN HOUR(time) BETWEEN 20 AND 23 THEN 'Evening'\n      ELSE 'Unknown'\n    END AS time_bin\n  FROM ny_statewide_2020_04_01\n  \n  UNION ALL\n  \n  -- Ohio Data\n  SELECT 'OH' AS state,\n    CASE \n      WHEN HOUR(time) BETWEEN 0 AND 3 THEN 'Late Night'\n      WHEN HOUR(time) BETWEEN 4 AND 7  THEN 'Early Morning'\n      WHEN HOUR(time) BETWEEN 8 AND 11 THEN 'Morning'\n      WHEN HOUR(time) BETWEEN 12 AND 15 THEN 'Midday'\n      WHEN HOUR(time) BETWEEN 16 AND 19 THEN 'Afternoon'\n      WHEN HOUR(time) BETWEEN 20 AND 23 THEN 'Evening'\n      ELSE 'Unknown'\n    END AS time_bin\n  FROM oh_statewide_2020_04_01\n) t\n\nGROUP BY state, time_bin\n\n\nAfterwards, I try two different methods of visualizing the data to see which would work better for comparing and contrasting the patterns between the three states. First, I try visualizing each state in a bar graph of their own, and put them side by side to compare and contrast the data from each state.\n\n\nCode\ntimeofday_tibble &lt;- timeofday_tibble |&gt;\n  mutate(time_bin = fct_relevel(time_bin, \n                                c(\"Late Night\", \"Early Morning\", \"Morning\", \"Midday\", \"Afternoon\", \"Evening\")\n                                ))\n\nggplot(timeofday_tibble, aes(x = time_bin, y = stops)) +\n  geom_col() +\n  facet_wrap(~ state) +\n  labs(title = \"Traffic stops by time-of-day (counts)\",\n       x = \"Time of Day\",\n       y = \"Number of stops\") +\n  theme(axis.text.x = element_text(angle = 25))\n\n\n\n\n\n\n\n\n\nMy second attempt is combining all the data into one bar graph, with three entries for each time bin, one for each state. The states are differentiated by color.\n\n\nCode\nggplot(timeofday_tibble, aes(x = time_bin, y = stops, fill = state)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Traffic stops by time-of-day (counts)\",\n    x = \"Time of Day\",\n    y = \"Number of Stops\",\n    fill = \"State\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nI find that its easier to compare and contrast the overall trends in each state when we visualize the data in three seperate graphs right next to each other. The combined graph becomes a little bit too noisy to be able to clearly pick out trends and patterns in each state, but it’d be useful if we wanted to directly compare the counts for each time bin between states rather then comparing the overall pattern.\nLooking at the overall data, we see a general pattern, and some interesting tidbits for each state. Generally, traffic stops are low from midnight til 8:00am, where it spikes and stays high until 4:00pm. where it starts decreasing. Ohio is the only state out of the three that maintains a high number of traffic stops during the late night time bin (12:00AM til 4:00AM), while Washington and New York have similarly low counts of traffic stops for both the late night and early morning time bin. New York, seems to have a significant dip in the traffic stops in the afternoon time bin for whatever reason, something we don’t see for Ohio and Washington.\n\n\nCode\nDBI::dbDisconnect(con_traffic, shutdown = TRUE)\n\n\nCredit:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "proj4.html",
    "href": "proj4.html",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "The ethical data dilemma examined here centers on COMPAS (Correctional Offender Management Profiling for Alternative Sanctions), a widely used machine risk-assessment tool in the U.S. criminal justice system. COMPAS, trained on large amounts of data, uses unique algorithms that chooses how to interpret that data and is used in all steps of the criminal justice systems in counties across the country, including the pretrial process, in determining who should be given bonds, who should be incarcerated, who should be given parole, etc (Angwin et al. 2016). The data component of this issue is about how this machine tool is created and maintained. Large amounts of data is needed to train COMPAS, and how the creator chooses which data to feed and train COMPAS off of, how the data is interpreted, which data is given weight and which isn’t are all very relevant components of data science. The ethical dilemma is very integrally tied to the data science component of this issue. In their current state, this risk assessment tool often have little to no transparency as to how they assess data or compute risk scores for defendants. Additionally, its validity as an “unbiased” tool is contested, and they have the potential to very easily exacerbate and increase the systemic discrimination present in the today’s criminal justice system of the United states (Chohlas-Wood 2020).\n\n\nThe for-profit company, Northpointe, benefits. They are allowed to profit off a tool that could be harmful on a systemic level by citing trade-secret claims (Angwin et al. 2016). The current oppressors and people who benefit from systemic discrimination benefits from this tool that exacerbates it. It matters because by adopting a tool that is reinforcing systemic discrimination, we are taking two steps back in trying to take one step forward. The ethical violations were because in the interest of profit. A lot of the criticisms would not apply to an open source project for a risk-assessment tool.\n\n\n\nWe don’t know how the variables that go into calculating risk scores are selected, cleaned, or chosen, and that is one of the primary concerns with the current state of this machine risk-assessment tool. There is little to no transparency with COMPAS under the guise of trade-secrets (Chohlas-Wood 2020), and no accountability for the creators as such. Because these tools are being run by a for-profit entity, Northpointe, they are motivated to mask their process and ensure the public does not know how their algorithms work (Angwin et al. 2016). However, that means Northpointe also face no accountability or responsibility for how they choose the data they feed and train their tool off of, nor for the systemic discrimination they may be exacerbating. Historical data of criminal justice often have extremely strong racial biases built in due to the flawed and discriminatory nature of the societal system of the time (Chohlas-Wood 2020).\n\n\n\nThe COMPAS tool is currently being used in ways contrary to the original intention. Tim Brennan, a former statistics professor and co-creator of COMPAS, has said that he did not intend for the tool to be used in courts, especially to decide punishment (Angwin et al. 2016). Yet this is often how the tool is used, as it is used in this manner in multiple counties such as La Crosse County, Wisconsin (Angwin et al. 2016).\n\n\n\nCOMPAS does not directly include race as an input, but it may indirectly capture racial information through correlated variables such as prior arrests or neighborhood factors. Including race is ethically problematic because race is a social construct with no biological basis (Morey 2023); using it risks reinforcing existing inequalities. In contrast, sex has and should be included because sex is a biological category with measurable differences between males and females. However, regardless of variable type, the absence of transparency about how these factors are weighted remains a major ethical concern.\n\n\n\nOne principle of responsible data science is the creation of reproducible and extensible work. The proprietary criminal justice tool named COMPAS violate this principle: its algorithms and documentation are inaccessible, preventing replication, peer review, or public oversight due to Northpointe’s profit motive. I believe this to be a large issue. A tool such as this used in our criminal justice system, which has large societal impacts for generations, should not be maintained by a for-profit company, nor should their development be curtailed because of a for-profit motive, when it should be peer-reviewed and open source."
  },
  {
    "objectID": "proj4.html#ethical-dilemma-machine-risk-assessment-tools-in-criminal-justice",
    "href": "proj4.html#ethical-dilemma-machine-risk-assessment-tools-in-criminal-justice",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "The ethical data dilemma examined here centers on COMPAS (Correctional Offender Management Profiling for Alternative Sanctions), a widely used machine risk-assessment tool in the U.S. criminal justice system. COMPAS, trained on large amounts of data, uses unique algorithms that chooses how to interpret that data and is used in all steps of the criminal justice systems in counties across the country, including the pretrial process, in determining who should be given bonds, who should be incarcerated, who should be given parole, etc (Angwin et al. 2016). The data component of this issue is about how this machine tool is created and maintained. Large amounts of data is needed to train COMPAS, and how the creator chooses which data to feed and train COMPAS off of, how the data is interpreted, which data is given weight and which isn’t are all very relevant components of data science. The ethical dilemma is very integrally tied to the data science component of this issue. In their current state, this risk assessment tool often have little to no transparency as to how they assess data or compute risk scores for defendants. Additionally, its validity as an “unbiased” tool is contested, and they have the potential to very easily exacerbate and increase the systemic discrimination present in the today’s criminal justice system of the United states (Chohlas-Wood 2020)."
  },
  {
    "objectID": "proj4.html#who-benefits-who-is-harmed",
    "href": "proj4.html#who-benefits-who-is-harmed",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "The for-profit company, Northpointe, benefits. They are allowed to profit off a tool that could be harmful on a systemic level by citing trade-secret claims (Angwin et al. 2016). The current oppressors and people who benefit from systemic discrimination benefits from this tool that exacerbates it. It matters because by adopting a tool that is reinforcing systemic discrimination, we are taking two steps back in trying to take one step forward. The ethical violations were because in the interest of profit. A lot of the criticisms would not apply to an open source project for a risk-assessment tool."
  },
  {
    "objectID": "proj4.html#data-collection-and-quality",
    "href": "proj4.html#data-collection-and-quality",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "We don’t know how the variables that go into calculating risk scores are selected, cleaned, or chosen, and that is one of the primary concerns with the current state of this machine risk-assessment tool. There is little to no transparency with COMPAS under the guise of trade-secrets (Chohlas-Wood 2020), and no accountability for the creators as such. Because these tools are being run by a for-profit entity, Northpointe, they are motivated to mask their process and ensure the public does not know how their algorithms work (Angwin et al. 2016). However, that means Northpointe also face no accountability or responsibility for how they choose the data they feed and train their tool off of, nor for the systemic discrimination they may be exacerbating. Historical data of criminal justice often have extremely strong racial biases built in due to the flawed and discriminatory nature of the societal system of the time (Chohlas-Wood 2020)."
  },
  {
    "objectID": "proj4.html#use-beyond-its-intended-purpose",
    "href": "proj4.html#use-beyond-its-intended-purpose",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "The COMPAS tool is currently being used in ways contrary to the original intention. Tim Brennan, a former statistics professor and co-creator of COMPAS, has said that he did not intend for the tool to be used in courts, especially to decide punishment (Angwin et al. 2016). Yet this is often how the tool is used, as it is used in this manner in multiple counties such as La Crosse County, Wisconsin (Angwin et al. 2016)."
  },
  {
    "objectID": "proj4.html#use-of-race-and-sex-as-variables",
    "href": "proj4.html#use-of-race-and-sex-as-variables",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "COMPAS does not directly include race as an input, but it may indirectly capture racial information through correlated variables such as prior arrests or neighborhood factors. Including race is ethically problematic because race is a social construct with no biological basis (Morey 2023); using it risks reinforcing existing inequalities. In contrast, sex has and should be included because sex is a biological category with measurable differences between males and females. However, regardless of variable type, the absence of transparency about how these factors are weighted remains a major ethical concern."
  },
  {
    "objectID": "proj4.html#alignment-with-data-values-and-principles",
    "href": "proj4.html#alignment-with-data-values-and-principles",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "",
    "text": "One principle of responsible data science is the creation of reproducible and extensible work. The proprietary criminal justice tool named COMPAS violate this principle: its algorithms and documentation are inaccessible, preventing replication, peer review, or public oversight due to Northpointe’s profit motive. I believe this to be a large issue. A tool such as this used in our criminal justice system, which has large societal impacts for generations, should not be maintained by a for-profit company, nor should their development be curtailed because of a for-profit motive, when it should be peer-reviewed and open source."
  },
  {
    "objectID": "proj4.html#bibliography",
    "href": "proj4.html#bibliography",
    "title": "Bias in Machine Risk Assessment Tools",
    "section": "Bibliography",
    "text": "Bibliography\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” ProPublica. ProPublica. May 23, 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\nChohlas-Wood, Alex . 2020. “Understanding Risk Assessment Instruments in Criminal Justice.” Brookings. 2020. https://www.brookings.edu/articles/understanding-risk-assessment-instruments-in-criminal-justice/.\nMorey, Rajendra. 2023. “What Is the Biological Basis for Race - Implications for Psychiatric Genetic.” European Neuropsychopharmacology 75 (October): S47–47. https://doi.org/10.1016/j.euroneuro.2023.08.095."
  },
  {
    "objectID": "tidytue2.html",
    "href": "tidytue2.html",
    "title": "Wealth Inequality After Taxes",
    "section": "",
    "text": "The visual below shows the change in Gini Index across selected countries after taxes and government benefits over multiple years. The change was calculated by subtracting the post-tax Gini Index from the pre-tax Gini Index. Since a higher value of Gini Index indicates a higher level of wealth inequality, a positive change means a lower level of wealth inequality post-taxes and benefits.\n\n\nCode\nincome_inequality_processed |&gt;\n  mutate(gini_change = gini_mi_eq - gini_dhi_eq) |&gt;\n  filter(Code %in% c(\"USA\", \"BEL\", \"LUX\", \"ISL\")) |&gt;\n  group_by(Entity) |&gt;\n  ggplot(aes(x = Year, y = gini_change)) +\n    geom_bar(stat = \"identity\") +\n    facet_grid(~ Entity, scales = \"free_y\") +\n    labs(\n      y = \"Gini Index Change\",\n      x = \"Year\",\n      Title = \"Yearly Gini Index Change Across Selected Countries\"\n    ) +\n    theme(\n    text = element_text(size = 40),      \n    axis.title = element_text(size = 40),\n    axis.text = element_text(size = 20)\n  ) \n\n\n\n\n\n\n\n\n\nCredit:\nData Processed By:\nhttps://ourworldindata.org\nSources:\nhttps://www.lisdatacenter.org/our-data/lis-database/\nhttps://www.oecd.org/en/data/datasets/income-and-wealth-distribution-database.html\nhttps://public.yoda.uu.nl/geo/UU01/AEZZIT.html\nhttps://www.gapminder.org/data/documentation/gd003/\nhttps://population.un.org/wpp/downloads?folder=Standard%20Projections&group=Most%20used\nhttps://github.com/open-numbers/ddf--gapminder--systema_globalis\nTidyTuesday Link:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-08-05/readme.md\nData Curated By:\nJoe Harmon and the Data Science Learning Community\nhttps://github.com/jonthegeek"
  }
]